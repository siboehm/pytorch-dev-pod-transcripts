---
layout: post
title: "Strides"
date: 2022-05-29
---
The original audio files are available at https://pytorch-dev-podcast.simplecast.com/

# Strides

Hello, everyone, and welcome to the Pytorch Dev podcast.
Today, I want to talk about strides in Pytorch.
This topic I have blogged about before, and I've also written a little bit about it.
But given that Mike Ruby has recently raised a proposal, for stride agnostic prim torch semantics, I thought it would be a good time to talk about what is meant by strides and some of the interesting characteristics that matter when you're dealing with this concept.
Okay.
So what is a stride? Well, a stride as its name suggests simply says how much you need to go to find the next element in some memory.
So remember, when we represent tensor data and these are these multidimensional phase.
It's actually not fully specified how exactly you map a coordinate like a a set of indices to an actual location in memory.
Now with a one dimensional array, You might imagine that you you represent it in a very normal way, which is that while you have your first element at the location, at the start of your ray, and you find the next element by going to the next slot and so forth and so forth.
And we would call that having a stride of one because you just go to the next position one over in that situation.
But you're not limited to only being able to do that.
For example, let's suppose we have a two dimensional array say it's a five by five array.
Well, to go to the next element in a row, you would still go forward one in memory.
But what if you want to go to the next row? Well, in that case, you wouldn't be able to find the next element one over.
You would have to go five elements over skipping past all of the elements that were stored for the first row to get to the first element of the second row.
So going by column, it's stride one, but going by row, it's stride five.
This is a lot easier to visualize if you've actually got a diagram in front of you So I highly recommend checking out one of my blog posts.
You can find it inside the podcast description to see a little bit more about you know how exactly this works.
but essentially all the stride is is it's a specification for any given dimension in your tensor how far you have to go in memory to find the next element there.
And so typically, you know, the innermost dimension, the one on the right, when you're talking about, like, the size, that's gonna have a very low stride like one.
And then the outermost I mentioned, the one on the leftmost side like, the first dimension, it will tend to have the biggest stride because, well, you've got to get past all of that other data, over to the right hand side before you can actually get to the next whatever it is in your dimension.
Okay.
So that's what strides are.
And mathematically, when you think about how to index a no tester, it turns out your index cleaning formula is very simple.
You just take you know, the index zero and multiply it by the strived for zero, plus the index for one, multiply it by the strived for one, and so for this for so So it's a very simple formula, and it's pretty easy to implement.
Well, unless you have a arbitrary-dimensional energy tester, And, you know, that's sort of where we come from with Pytorch.
Strides have been with Pytorch since before.
Pytorch was even a thing, torque the library that Pytrux is derived from also had strides.
Stars are pretty useful and there are two primary reasons Why Pytorch? And also, NumPy.
NumPy also has a concept of strides, support it.
And those two main reasons are views this is the original reason we had strides.
And the second reason memory formats, which was added on at a later point in time during Pietro's history.
Let's unpack these two use cases.
So what do I mean by a view? Well, notice that I was talking about how to find the next piece of data.
Right? And I said, well, you know, if I'm looking for the next element in the row, I just go by one and for the next element in the column, I go by five.
And so when you are talking about tensor data, sometimes you want to talk about a subset of the tensor data and treat it as a tensor in its own right.
For example, you have a two by two sorry, a two d matrix, and you wanna extract out a row from that matrix.
Well, extracting out rows is pretty easy because all you need to do is you just take whatever your offset is.
where the row starts.
So if it's the zero row, you'll start at beginning.
But if it's the, you know, fifth row, you'll start at, you know, memory location twenty five, say, and you just, you know, adjust the length so that you only see that row.
And so if even if you don't have any consider strides, it's very easy to represent, you know, sub rows, sub rows of a tensor in this way.
And so if you were, like, doing stuff with c plus plus vectors, for example, there's a very handy utility class we have in patterns called arrayref, which is a non owning view onto vector And you can you can do this.
You can have it take out an array ref to a arbitrary row in a, you know, virtual two d vector.
where you just have everything stored continuously instead of having a vector of vectors.
Okay.
So that's very easy.
But what if you want to say, for example, return a tensor that represents a column of your of your tensor.
Now that is not going to be so easy because if you look at each of the individual elements, they're gonna be laid out in memory differently than on the row.
The row, all the elements were one after another, you know, move one, find the next one, move one, find the next one.
But for the column, we said, well, you have to move five to get to the next one.
So they're not together.
They're so called non contiguous.
And so in this situation, if you only had the ability to say, well, here's where you should start reading, and then you should just read out a contiguous chunks of data that's and long, you would have no way of representing a column without actually copying out the data so that it's contiguous.
And so when you have a tensor representation that supports strides natively and that's what Pytorch has and that's what Nuhnn has, we have the ability to represent column tensors without doing any copies because all we do is we say, okay.
Well, let's have a one dimensional tensor.
We'll start it at the beginning of the tensor, but instead of having the stride b one, which would be the normal situation with a one d tensor, we would have a stride b five, say, so that I know okay to find the next element, I skip over five, and then go forth, and so on.
And now, of course, many operations in Pytruch only know how to handle contiguous inputs.
So when you do a, you know, strike a tester like this, sometimes there won't be any profit.
You're simply just delaying the in inevitable.
Once you actually do a computation on it, we'll go ahead and, you know, zip through the data gathering it together into contiguous tensor and then running the operation.
So it's the same as if you had done the copy ahead of time.
But there are two important differences.
in PyTorch.
So one is that because the view of the Tensor shares storage, right? We didn't do a copy initially.
The copy only happens lazily.
when an actual kernel needs to be run, if I mutate the original tensor or if I say mutate the view it will show up in the other place either the view or the original sensor depending on what you did.
So that's really handy because one of the things that, you know, is really nice about working with Pytorch is you can, you know, go and explicitly mutate tensors mutate views and use that to sort of initialize your tensor if you need to.
Now it's not recommended because it doesn't work well with auto grad, but it works and you can do it and that's very useful.
The other important thing about having views being represented in this way is that sometimes we are able to handle a noncontiguous input without having to do a copy.
So in that case, we've saved ourselves from having to do a bunch of data movement.
And instead, we, you know, we're able to fuse the gather operation directly into the kernel instead.
So one way that I like to think about views is they're a very limited form of lazy evaluation.
Right? Instead of doing the gather immediately, instead of doing the collection of all the columns elements into continuous tester, we defer it and we wait until the actual kernel needs to get run, and then that's when, you know, we figure oh, okay.
The kernel actually supports the situation.
Horae, otherwise, oh, maybe the kernel doesn't support the situation.
Oops.
By the way, so this is a little side note, but when you're writing colonels and PyTorch, you do have to think about whether or not your kernel is going to know how to deal with non contiguous inputs or not.
And this is actually kind of a pain because a lot of kernels don't really know how to deal with discontinuous inputs.
It's a lot of work to actually handle strides, and it makes your indexing formulas more complicated, and it makes your kernel slower.
because if your kernel can assume that everything is contiguous, then it doesn't need to do all of this indexing arithmetic, you know, all the multiplying strides by sizes.
to actually figure out where the location it's going to get out data from is.
So a lot of kernels just wanna assume I you know, I've just got a contiguous thing.
And so, historically, if you wanted to write code like that by hand, what you would have to do to be strictly correct is you would have to go and So okay.
So there's two situations.
So one is if you're writing a functional kernel, you would need to go and check if the input was contiguous and if it was not you would have to run contiguous on it to get the contiguous input.
There's some helper functions like expect contiguous, which help you do this without incurring a rev count bump.
when there's no ambiguity in this case.
And the other thing that they let you do is they let you let's say that you have a kernel that takes an out parameter.
Well, if the out parameter is strided, you're expected to be able to go ahead and directly, you know, respect that striding because, you know, hey, maybe it's some view and that view is alias with some other base ten sir, and the user actually did wanted the output of the computation to get scattered in this way.
So to actually do this, we have to first allocate a contiguous sensor which is going to be our output go ahead and run our kernel writing the data into the contiguous tensor and then finally scatter it out into all of the actual user requested output sensor in this situation.
And as you can imagine, this is very easy to forget to do and there's a lot of kernels that don't do this correctly.
Fortunately, if you're writing structured kernels, there's a very nice new piece of functionality by Yukio Sarachi.
where, basically, you can say, hey, my colonels can't handle strided outputs.
they can only handle contiguous outputs and you say set output contiguous and this will go ahead and handle all of the ensuring that the output is in fact contiguous under the hood for you and do the copy out to the real output if necessary.
So you can just write your kernel without worrying about this stuff.
So it's pretty handy.
You should use it if you're in that situation.
Alright.
So that's it about what strides are good for with the views.
Now, there's another thing that I said they're good for and that is a member a memory format.
What do I mean by memory format? Well, memory format refers to some conventions about where exactly physically we put data when we are talking about them.
So for example, you may have heard of the terms channels first and channels last.
What exactly is meant by these terms? Well, they refer to a very common layout question you have to decide when dealing with image data, which is Specifically, when you represent the image data, do you represent it as a as a two d matrix representing the image data and you have a copy for the red values, a copy for the green values and a copy for the blue values.
So just imagine, you know, three distinct images, monochrome images, each one of them representing their respective color, but the images themselves being contiguous.
Or do you represent them in a sort of bundled manner where you have the channels you you have the value RGB for the first pixel? than RGB for the second pixel and so on and so on.
And the difference between these two formats corresponds precisely to channels.
First, or channels last.
Channels first being CHW where the height and the width pixels have the lowest stride.
And then to get to the next channel, you have to do a big jump.
And channels last aka HWC, where the channel has the smallest chart.
So to get to the next channel, you only have to do a single step.
Now depending on your situation, whether or not you're on CPU or CUDA and so forth, whether or not you want to lay out the memory in channels first or channel's last order differs, and it also depends on what operations you're doing.
Sometimes operations are faster if you have channels first, and sometimes they're faster if you have channels last.
Historically, Pytorch's memory foreign convention is that we always do channels first.
So whenever you have any APIs that take in data that is supposed to represent images, we'll always take them as n c h w, n being the batch dimension, c being the channel, and then the height and the width.
So what if you wanted to actually run some code that actually handled them with a channels last memory format.
Well, to do that, strides come to the rescue.
So the NCHW layout is what we think of as logical layout.
It just says, you know, when you're accessing the tensor from the user program, you know, the h and the w dimensions are the second and the third.
well as two index and the three index and then the channels, the one index dimension.
So if you want to actually change the physical layout in memory, so that the, you know, channels layout is index three.
All you need to do is set your strides appropriately.
So you can keep the same logical layout.
And so even when you're doing channels last, our channels first, you always see a n CHW tester as far as your concern from a user.
But by modifying the strides underneath the hood, we can change the physical layout so that it's actually laid out with channels first or channels last.
So this is how we support memory formats in PyTorch.
You we don't have the thing that TensorFlow does where there's an extra flag you have to pass to say convolution, saying, oh, channels are, you know, in the beginning position or they're in the end position.
Instead, we always assume channels are in the channel's first position.
And if you want a different memory format, you just modify the strides to get there.
Cool.
Okay.
So what is going on then, given all this information, what is going on with Mike's proposal for stride agnostic semantics.
Well, this comes down to the fact that although strides are very mathematically simple to express, That's to say you just multiply the size by the stride, and then you do that for every dimension in your tensor.
This actually leads to a little bit of too much leeway in the representation for strides.
In particular, let's suppose that I have a tensor and I have either dimension whose size is a zero or its size is one.
When a size dimension size is zero, You can see that the stride in this case doesn't matter.
Why doesn't it matter? While you have no elements in the tensor at all, so you're never going to try to ask for the next element because there's no elements at all to ask for.
So I can put whatever I want in the stride of a size zero tensor.
In fact, I can do whatever I want for any of the operations because I will never ever get called out in this situation.
Now, although you can put whatever you want as far as the indexing is specified, you may not want to do that for memory format because remember, our memory format is telling us whether or not this is a channel's for last or a channel's first tensor.
So, you know, having the strike set up correctly for this case, it does matter sort of, I guess.
similar situation shows up when you have a size one tensor.
Right? So once again, there is an element this time, so that's great.
But you're never gonna ask for the next element because there's only one element.
so you, you know, can have whatever you want in the strike once again because you're never ever gonna observe it in the situation.
Because the strides are over specified in this way, we do have a convention for what the contiguous strides of attempts are supposed to be even the zero and the one case.
But when you ask if a tensor is contiguous, we actually accept all of the possibilities for zero and one.
So basically, we know that there's flexibility here and we don't hold it against you if you pick something else in that situation.
That means that there's a lot of flexibility for what kernels choose in this case and they often just do whatever the heck they want in these situations.
So that's kind of annoying and when we are doing stuff like prim torch where we're trying to reimplement all of high torch directly in Python.
This is a pain because the way we do testing is we go ahead and, you know, run the original implementation and run our new implementation and check of the strides match up.
And while lo and behold, sometimes they don't because, you know, there's these degrees of freedom and they let the strides, you know, wobble in a way that does and actually matter.
Okay.
So I told you that the strides are over specified in some situations because of these degrees of freedom from size zero or in size one.
But wait, it gets worse.
So remember what I told you about memory format.
Right? So you have these n c h w and n h w c tensors.
And, you know, depending on having one or the other, your kernels might run faster or not.
So one of the things that we need in the situation is if you want to actually be able to run your network in NHWC, for example, that's the non default situation.
you need operators that actually propagate this NHWC format through the entire network.
So if I do an addition on a tensor and it's NHWC.
It better stay NHWC because there might be a convolution coming up afterwards that actually, you know, could have benefited.
from that other ordering.
Now, there's a problem though, which is that when we write up the rules for how strides should propagate, we have a very complicated situation what if a user passes us a tensor that is NAC n a NCHW and another tensor that's NHC.
that is to say their memory performance disagree.
What do you do in this case? Who knows? We have some algorithm for determining what exactly we should do in this situation, but it's you know, kinda complicated and hard to describe, and most people just don't close their eyes and, you know, hope something reasonable happens in this situation.
And so once again, you know, in Mike's proposal for stride agnostic.
He's basically saying, hey, you know, it's a lot of work to mimic this stride behavior and, you know, what are we even getting out of it? So since this is my podcast, I get to, you know, harp on what I think that what we should do in this situation.
So I agree that it's probably not a great idea to spend a lot of time worrying about what exactly happens when you have a tensor that is you you add to, like, a channel's first sensor and channel's last sensor together.
That just means you've done something wrong.
And, you know, that's fine.
and we shouldn't force ourselves to make sure that the semantics exactly match in this case.
But as I've mentioned before, we do use strides for two very important use cases, views and memory formats.
And so although, you know, maybe strides in their full glory, it's just too much for our puny little brains to deal with, we should make sure that we do have a good story for at least the two use cases we care about.
that's everything I wanted to say about Stride today.
See you all next time.
.
