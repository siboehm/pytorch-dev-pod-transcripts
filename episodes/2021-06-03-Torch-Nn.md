---
layout: post
title: "Torch Nn"
date: 2021-06-03
---
The original audio files are available at https://pytorch-dev-podcast.simplecast.com/

# Torch Nn

Hello, everyone, and welcome to the Pytorch dev podcast.
Today, I want to talk about torch dot n n, Pytorch's public API for actually building neural networks.
Of course, if you are a user of Pytorch, torch dot n n is one of the very first things you actually learn how to use and there are lots and lots of documentation about all sorts of ways to use modules in Pytorch.
And as this is a dev podcast, I'm not gonna really talk about how to use Torched Nen so much as, you know, if you are a maintainer or a potential contributor to the library, and you want to make modifications to torch dot n n.
Well, what are the kinds of things you're gonna have to worry about? What are some of the philosophies behind the design of this component? etcetera.
So let's dig in.
So torch on n n, as I said previously, provides the n n module abstraction.
Most importantly, which is how most people put together their deep learning modules.
Why does Twitch dot n n exist? Well, it exists because when you are setting up your modules, model.
You have a lot of computations that you wanna do.
You have a lot of parameters, and you need a convenient way to keep track of all your parameters.
because for example, when you are doing optimization, you need to iterate through all your parameters and, you know, apply the gradient you computed for each of them to the result.
And so if you're a purely functional person like in jacks, actually having to, like, manually keep track of all your parameters in, you know, like, a global spot in your application gets kind of annoying when your model gets very big.
And so what torch dot n n does is it gives you a convenient object oriented like interface that automatically can collect up all of the parameters for you so that you don't actually have to keep track of it yourself.
You can just ask, hey, what are the parameters of this model and they'll tell you all of them? Pretty cool.
Right? Another thing that is really important about torch dot n n is unlike many of the other pieces of Pytorch which we've moved to c plus plus, because, well, you know, c plus plus is faster.
We've tried very hard not to actually move torch dot n n to c plus plus.
And so if you crack open the Python files in Pytorch itself because, hey, you know, how is convolution implemented? Well, it's still a, you know, plain old Python class that you can, for example, copy paste into your own project and tweak however you need.
And so another reason why touch dot n m is in Python is it's more hackable.
Right? Like a lot of times you are, you know, doing something that someone has done before, but maybe with some tweaks.
And there's nothing wrong with copy pasting code and research code.
It's probably the fastest way to get going.
and, you know, long term maintainability isn't as much of a concern.
And so we wanted to make sure this was still something that people could do when they wanted to do those things.
Of course, getting all these features to work ends up being pretty complicated.
So if you've ever cracked open module dot pi, the module that actually implements module for real, it's actually really, really long and there's tons and tons of stuff going on.
So let's just talk about the most important things that it's doing.
So one, I said that modules are able to collect all parameters.
how do we know if something is a parameter or not? Well, in PyTorch, there's a parameter sub class of the Tensor, which is how you make this distinction.
Right? So anything that is a parameter and you put it into a module, we will keep track of it anything that's not a parameter, just a plain old sensor, we won't keep track of that.
In order to keep track of all the parameters you put on the module, we need to override the behavior of what happens when you modify fields on your modules.
So most modules override behavior of set adder and get matter to basically say, hey, when you set an attribute on my module, is it a parameter? And if it is a parameter, then we actually just go ahead and, you know, put it in our record of all the parameters that are on the module.
So that's another piece of, like, complication inside the elimination of module.
Some other thing modules need to support, while modules support being transitioned from one device to another, traditionally the way that you like allocated module on CUDA is you first allocated and then you run dot CUDA on it.
another thing that modules need to support how to do is, you know, find all of the things in the module, all the tensors and not just the parameters, but also other buffers, and also any recursive sub modules that are also part of this module and also make sure things get called on them.
And so there's a, you know, little helper function called underscore apply, which knows how to iterate over what essentially is every tensor in the module and apply an operation to each occurrence of it.
Another thing that modules implement are hooks Hooks are ways of just interposing in on the behavior of modules without having to manually write in code in every location.
And to implement this, well, you know, when you define a module, you write a function called forward.
But when you wanna actually invoke a module, you don't call the forward function directly, you call the operator call, like underscore underscore call, like just a plain old function call on the module directly.
And that call does a bunch of work.
It like processes hooks and figures out all the sort of administrative stuff before actually calling the forward implementation.
do the actual thing you want to do.
So there's a lot of goop in module dot pie, but, you know, if you just keep these three things in mind, right, like we need to keep track of the parameters, so there's overriding behavior of Set actor and getator.
There's implementations of these functorial operations, which operate over all the tenses on the module.
And then there's a bunch of folks in in your position that, you know, let people tweak the behavior of modules without having to edit them manually.
you'll actually, you know, be able to understand a good majority of the lines of code in modelled up high.
There's really only two other things you have to worry about.
One is serialization.
Right? Like, a really important thing to be able to do is once you have your module and you have trained it, you wanna dump all the parameters to disc.
so you can use them them them again later.
Well similar to how we keep track of all the parameters, there's also a notion of sets of things that actually get persisted when you serialize a module.
The recommended API for doing this is state dict, which just gives you a dictionary mapping from key names to tensors that says all of the parameters in question, you can also technically pick all the module directly, although this is a lot more fragile because Pickling requires you to actually maintain exactly the same name of the module and exactly the same module that the module is defined in module in the Python module sense.
One last complication when writing modules in Pytruch itself is most modules in Pytorch are what we call torch scriptable.
What's torch script? Well, torch script is our compiler.
for Pytorch models.
And essentially, what it lets you do is if you have a torch go to model, you can translate into torch scripts intermediate representation.
and then you can, for example, ship it in a, like, Python agnostic form or you can also run some optimizations on it.
And because JavaScript is a compiler, but Python is really complicated, there's some restrictions that apply when you want to write modules because you need to make sure they're actually towards scriptable.
The most obvious restrictions are that there's a limited set of types you're allowed to use because the interpreter in JavaScript doesn't support arbitrary types.
And you also have to make sure that the set of Python you use inside your forward function is the set of Python that is actually understood by JavaScript.
Although, JavaScript actually does support a lot of Python feature So chances are normal things you do are going to be understood.
What are the more unconventional things about how torch grip compiles modules It's it's actually a staged computation.
So when I imagine compiling an n n module, you could imagine compiling an n n module including the constructor and the forward implementation.
But that's not actually how JavaScript works.
What JavaScript does is it first instantiates the module as a normal python, so you actually construct the module.
And only once you've constructed the module, do you actually then attempt to compile the forward implementation on it? There are some benefits to doing this.
In particular, because the initialization of the module happens in ordinary Python, you can go wild with anything you want in this case, and, you know, there's no restrictions on the initialization code for the modules.
You can do anything you want.
And furthermore, once you've actually initialized all the attributes on the class in question, JavaScript has a much more accurate picture about what the actual parameters on your class r.
So if you have some weird situation where, you know, if you pass in a parameter and it's true you allocate a parameter And if it's false, you don't allocate the parameter.
Well, Torescript can handle this fine even though Torescript is statically typed and you need to know exactly what all the fields on your module are.
So that's some of the things you have to be aware about when you're working on modules in NN module.
What else? Well, there's been some new developments in an end dot module.
Shockingly, I know because everyone and their dog subclasses from modules.
So when we make changes to the class, we have to be very careful because there's a lot of people who will be very unhappy with us if we ever break backwards compatibility.
on modules.
That being said, we've been able to come up with some new things that like make modules easier to use.
One of the coolest new additions is the concept of lazy modules authored by Ian Castillo from preferred networks.
What lazy modules do is solve a common problem that you have when you're trying to construct a model, which is that you don't know how big the parameters should be.
because you know what's going on while you're passing in some input of some known input size and it's going to your model and at some point you're like in the middle of the model and you need to provide an f c layer.
And that f c layer needs to know how big the input is because the parameter in question is gonna be, you know, the size of the input times the size of the output.
But you have no idea what the input size is gonna be.
Like, you know, you've run a pile of convolutions.
Who knows what the result is gonna be? and you don't wanna have to manually, you know, compute what the size is at that point in time.
So prior to lazy modules, you had to suck it up and, like, add some print statements to figure out what it was.
But with the lazy module, you just say, okay, well, lazy f c with what the output size is supposed to be because that's not specified.
And then the first time you run the forward on the module in question, it says, hey, this input is size x.
Okay.
Now I'm going to evaluate the now I'm going to allocate the parameter because I know what the size of the input is.
Another really interesting recent development is for the longest time you couldn't actually allocate a module directly on CUDA.
And so we forced everyone to, like, allocate on CPU first and then move it to CUDA.
This wasn't too bad when models were small, but people are really excited about really big models.
And sometimes the models are so big, you can't even fit them on a single machine.
So how the heck are you gonna construct a module in that case when it's too big to fit on your machine? So what Joel Schlosser has done is he's added a new device keyword argument to all the modules in Pytorch.
So what does this mean? So if you are constructing a module in Pytorch, and you pass in device equals CUDA when you construct it instead of constructing a module on CPU and then moving it to CUDA, what it will do instead is it'll directly construct module on CUDA.
This patch was super simple.
Right? All we did was like edit the initialization code to actually respect the device.
But, you know, I don't know why we hadn't done it before, but, you know, Joel actually made it happen.
And we're hoping that throughout the rest of the Pytronch ecosystem, people will start following this convention.
And so given an arbitrary module, you can just passing device and get the module on the device in question.
One of the, like, cool interactions with this other feature that we've been working on called meta tensors is if you say device equals meta, what you'll get is you'll get a constructed module but all of the tenses will be not allocated.
They'll be meta tensors saying what their sizes are, and then you can do post facto analysis on it in this situation.
One of the open questions for us with the NN module design, there's a few things.
So one problem that is coming up for us soon is we actually do need some sort of functional version of modules because sometimes you're doing sort of higher order training or you're doing APIs that only work on purely functional programs.
And in those situations, like, the very stateful nature of Pytorch engine modules doesn't work so well.
So that's one thing.
Like, given a module, can we turn it into its functional version.
Another open problem that has been plaguing us for a while is many of the weight initializations in Pytruch are very out of date, Like, they basically harken all the way back to Louis George days.
And the research has gone beyond and figured out that there's better ways to initialize in these ways.
And we're stuck in a hard place because, well, on the one hand, we like to update the initializations.
But on the other hand, if we do that, lots of people's, you know, pre existing models might break because while they maybe expected some particular initialization.
We have some ideas about how to fix this like imagine some sort of like version that you can specify, hey, I want weight initial itemization, version three, and that comes with all the updates and you just explicitly opt into it.
But no one has really implemented this yet and something I'm kind of interested in seeing done at some point.
That's everything I wanted to talk about and module today.
Talk to you next time.
.
