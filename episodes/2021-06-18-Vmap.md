---
layout: post
title: "Vmap"
date: 2021-06-18
---
The original audio files are available at https://pytorch-dev-podcast.simplecast.com/

# Vmap

Hello, everyone, and welcome to the Pritchard Step podcast.
Today, I want to talk about vmap.
vmap is a feature that was popularized by Google Docs, which lets you write code without thinking about batching and then automatically make your code batched.
So let's imagine that you want to, you know, add tensors together and then do a matrix multiply on them, maybe run a convolution on them.
What VMAP says is you can write it as if you were writing this computation on a single batch, really no batch dimension at all.
And then you can v map over it to so called vectorized it.
That's what the v and v map stands for so that all these operations transform into their vectorized operations.
And in many cases, many ops in both PyTorch and NumPy and JAKs are automatically batched in the sense that if you at tack on extra dimensions to the beginning of the tensor, the operations semantics will say, okay, I'll just treat those as batch dimensions and process them.
There are a lot of operations that don't do that.
For example, operations that only take a single batch dimension or operations that change their meaning when you add more or less dimensions.
Matrix multiply is a particularly bad offender in this front.
And vmap makes it so that you don't have to, like, worry about, oh, yeah.
If I wanna add a batch dimension, I can't use Matt Moll anymore.
I have to use BMM instead.
And all it says is, no, just write the single example version and I will automatically translate it into the batch version as necessary.
So how do you implement vmap? There are a number of different ways, but I'm going to talk about the particular implementation that Pytorch's v map implementation uses because it's the one I know best and it's most relevant if you want to develop Pytorch.
So in Pytorch, when I want a v map over a tensor, what I do is I introduce a new concept called a batched tensor.
a batch sensor can be thought of a regular sensor but with some of its dimensions marked as being so called batch dimensions which don't nor participate in normal computation in the way that you normally imagine.
So let's imagine that I'm talking about a square matrix, right, you know, a by b, and I want to batch it.
So I have a batch dimension on it.
So ordinarily in PyTorch, if I asked, what is the dimension of this, you know, batch by a by b tensor, I would tell you three.
But with a batch denser, the batch dimension is considered a private implementation detail, and so I don't get to see it.
So if I ask what the dimension of a batch sensor with one of these batch dimensions in it is, I actually only get two because logically, when I'm looking at this sensor, I wanna just be able to do single operations on it.
And so when I say, hey, what's the size of it? I should see only a single instance in question.
But under the hood, What the batching tensor is doing is it's translating your operations on this domain, this single example domain into the multiple example domain And that's why, you know, we we do need to have a tensor that stores all of the various batches in question.
You just don't get to see it as a user.
This distinction between logical and physical dimensions is very helpful because it helps you sort of keep straight what is going on in the logical universe, namely what you see as a user, and what is going on in the physical universe okay, what operations are actually happening? So give another example.
When you wanna do a sum, a reduction some, you can say what dimension you want to do the reduction on.
Right? So let's imagine once again you've got a two dimensional tensor and you wanna do a reduction on the first dimension.
So dim equals zero.
So if you have a tensor that's, you know, A y b, you just say, okay, some open parenteral equals zero, and then I'll do the reduction in the first dimension.
But what if this sensor gets batched? Well, if this sensor gets batched, then it's not correct to write dimming equals zero to reduce the zeros logical dimension because what that'll do instead is reduce over the batch dimension.
And those of you who have seen some of the marketing copy for named tenses may recognize this as, like, a similar problem that named tenses were were trying to solve for So name Tensor's answer to the solution is, okay, don't say that you wanna do a reduction over dimension zero.
say that you wanna do a dime reduction over the height dimension, let's say.
What v mAb says and said is No.
No.
No.
No.
You can still use numeric designations.
We just won't actually ever, you know, make the batch dimensions visible to you.
So you can say, oh, I wanna reduce over dimension zero.
And if you have a a y b sensor, that'll be a, you have a batch by a by b sensor that'll still be a.
You have a batch one by batch two by a by b sensor, it'll still be a.
And the v map process will adjust the index so that from the logical idea of zero, to three or four or whatever it needs to be depending on where you've inserted the batch dimensions when you're doing the actual interpretation on the inside.
And really, that's all there is to it to the v map implementation in PyTorch.
So we have a v map dispatch key.
You don't know what dispatch keys are.
go listen to one of my earlier podcast about the dispatcher.
We have a v map dispatch key, which interposes in on v map when you wanna do an operation.
And when you have one of these batch tensors which get created when you use the v m app operation.
Right? So when you v m app over a tensor, On the inside of the v map, we'd give you a batched tensor, which will do the batching for you.
And we when you we do you hit the VM app to search key, it does the translation from the logical into the physical thing in question.
And then, you know, it re dispatches.
And the physical operations just get handled in the same ordinary way.
you used to see them handled.
Another way I like to think about this problem is that I'm doing these sort of functional transformation on my API calls.
And this is this is very much the Jack's interpretation for vMAb, which is that I've got my program It has all of these calls to add mall, Matt Mall, whatever.
And what the v map call does is it transforms this into a corresponding vectorized program, v ad, v mall, vMM, assuming those were actually operations, which they which they typically aren't.
But, like, if you had a vectorized version of Ad and a vectorized version of Matmall, You just translate to those versions, but otherwise your program stays very similar.
So like like in a sort of very mathematical sense, you're in this sort of world of single example functions.
And there's this extra world of multi batched functions.
And there's a mapping of every function in the sort of single example world into the batched world.
And so as long as you, like, say how to do this translation, then you can just take your program of single example calls and then project it into this other world.
And, like, if you are a Haskeller, you'd call this a type of functor.
It's not a functor on Haskell per se, but it's a functor on on valid tensors.
If that didn't mean anything to you, don't worry.
But, like, the picture that I want you to have in your head, right, is you're taking all these function calls and you're replacing them with vectorized function calls.
And you might do this multiple times if you, for example, vmat multiple times.
This looks pretty different from the physical implementation.
Right? because the physical implementation keeps track of what batch dimensions are on tensors.
And what it does is it actually, you know, it's a little more efficient.
It, like, collapses all levels of v maps into a single batched sensor, but there's another implementation you could have done for v map.
which is you have a single batch sensor which hoses a single batch dimension and you just repeatedly wrap each time.
And so if you, you know, did a v map of a v map of a v map, you would end up with batch tensor containing a batch tensor containing a batch tensor which contains an actual tensor.
And so in this way, you can think of this sort of like as you've got this chain of control where The first call hits the top level batch tensor, which does a transformation and then transforms that operation into a vectorized operation which then passes into the second batch sensor.
And then when you well, the second batch sensor is asked, hey, I got this vectorized operation Can you vectorize it again for me and you end up with a vectorized, vectorized operation and so forth and so forth until you bottom out and there's no more batching to be done.
By the way, this is what when Jack says that its functional transformations are composable, this is what is meant, which is that when you apply the transformation to the operation, you get back a thing that you can apply the transformation again to.
It's like a it's an endo functor in other words.
And it's really profitable to realize that even if the implementation involves these like batch tensors and, you know, they're doing all this bookkeeping and they're intercepting operator calls.
It's really helpful to think about the actual semantics as just morally replacing these operations.
So whenever, like, I'm in a situation where I'm like, I'm not sure what VMAP is supposed to do in this case, instead of like trying to run a batch denser like object in my head, instead I just think about, oh, well, you know, like, what would I, like, modify these API calls to look like when I did it this way? And that usually tells me what I wanted the behavior to be.
So to give an example of this, a classic problem when you're doing b mapping is how to handle random number generation.
So let me explain what the problem is.
So let's say that you're doing a v map and at some point during the v map you make a call out to a random number generator.
So you like say torch, random, give me a buffer filled of random numbers, and then maybe say add it to one of these batch tensors.
And so there's a problem which is what do what is the semantics of this? For each batch in the batch tensor, do I separately generate random numbers and then, you know, pertur them all differently.
So this is like sampling noise and then you'd want the noise to be different across batch dimensions.
Or am I sampling the noise once and then applying the same noise to every batch in question, sort of shifting everything exactly the same way.
And so there is probably something that the naive implementation of your code would do, that is to say replicate the random numbers in each case.
But that's not a good way to think about what you actually want the semantics in the situation to be.
Right? So if we think a little bit further and we say, okay, Well, you know, what kinds of transformations to the API calls do I wanna have happened in this situation? We quickly see that The replicate the noise the same way everywhere corresponds to when I don't modify the random number generation calls.
So I just do a plain old stock random number generation call.
I modify the add into a vectorized add, and what that is going to do is broadcast the random number generator which we call wasn't modified at all.
So it's gonna be made at the logical size, not the physical size.
And that broadcasting is what causes the random number generation to be reused for every batch.
Where as The case where I do a new random number generation for every single batch corresponds to transforming the random call into a call that has a batch dimension, and then I don't have to do broadcasting when I add things together later.
And so there's two reasons why this is a really useful way of thinking about it.
So one is that it gives you a way of thinking about how you might actually implement this.
and the way you can implement this is by doing a mode key.
So normally, the problem is is that dispatch and pie torch is based on the types of tense And so random has a hard time dispatching to batching batch tensor v map because it doesn't take any tensors as input.
So it doesn't know oh, what the v map should be.
And we have a way of working around this, which is a so called mode, which is, hey, when you turn on this mode like AMP, automatic mix precision, see previous podcasts, all operations are affected by this even if, you know, there's no input dependence at all.
In Jack's, this is called omni staging if if you were curious.
So if you make a v map a mode, then you can interpose in random and then like look at what the state of your, you know, v mapping is and then, you know, generate the random appropriately.
And this is pretty nice because it turns into sort of the common way to fix this ambiguity, which is if you wanted the random number generation to be generated once, per all the batches.
Make sure you generate it before you actually call the v map.
So make sure you call it outside of the v map.
And if you call it inside the v map, we're just gonna assume that you wanted the random number generation generated a new every time because while you're doing it inside the example in question, and that that maps very nicely to the mode cell implementation.
Jack solves this a little differently.
They force you to pass an explicit random number generator object to disambiguate these cases, which does disambiguate the cases and is more expressive.
But if you're like a very immutable person, moving things before and after function calls, sort of makes sense as a way to control when effects should happen.
It's like flipping a coin.
Right? Like, if you want to flip a coin once or you wanna flip a coin many times inside of a loop.
You well, you would either, you know, flip it once outside the loop or you would move it inside the loop to flip the coin many times.
So, you know, the analogy of vMAb as a loop also works here.
even though there's side effects involved.
So what are some things that are wrong with the current implementation of vMAb in PyTorch.
So there is one big problem which is that it is not fully composable.
So vMAb is set up in a way that it is composable with itself So we can v map as many times as you want, and batch sensor knows how to handle this.
And it composers with autograd in one specific way.
namely, if you wanted to v map your code and then run autograph on it, that's okay and that's supported by PyTorch.
and this is because dispatch keys have a fixed order so you can't reorder them.
Now the problem is sometimes you wanna run Autograph first and then v map over the autograph.
And this is very useful for doing this thing called per sample gradients.
which I'm not gonna explain in this podcast, but you can look it up if you're interested in it.
But composing them in this different way and know it's not the same thing these these operations are not commutative.
So, like, whether or not you do VMAT first and grader, grad then VMAT has implications on the performance of your code.
So to solve this, Richard Zo, the original author of v mab Invitouch and Horace had been working on a new version called Funktorch where instead of being forced to have a fixed order that transformations like this are applied in Pytorch, bash than v map, Instead, you just have a stack of transformations like Jack's.
Funktorch is unabashedly taking a lot of inspiration from Jack's and let you compose them in whatever order you like.
And that's pretty cool.
And, you know, Jack's has a lot of good ideas there.
There is a good thing about our implementation though.
Right? Which is that because we compress all v mAb layers into a single representation, we have to go less loops through like the translation because we can just do the translation all in one go.
It makes our batching rules a little more complicated but it reduces the sort of fixed overheads in question.
And so for PyTorch, we do care about this because we're an e remote framework.
We don't usually ask people to use a jit combinator to like get rid of all these fixed overheads.
So there's still a utility to this, but sometimes you do want like wild flexibility and then being able to compose things in whatever order you want, however you like is a useful capability.
So I hope I've explained a little bit about how vMAP is implemented and some of the various ways that I think about vMAP and also other sort of sorts of functional transformations in Pyturg.
By the way, there's an old podcast about functionalization.
You can also think of that.
as a functional transform in the same sense as vmat.
That's everything I wanted to say for today.
Talk to you next time.
.
