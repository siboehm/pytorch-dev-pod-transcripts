---
layout: post
title: "Batching"
date: 2021-08-17
---
The original audio files are available at https://pytorch-dev-podcast.simplecast.com/

# Batching

Hello, everyone, and welcome to the Pyturbine podcast.
Today, I want to talk about batching, a fundamental concept in Pyturbine and many other numeric computing libraries.
Batching is one of those very fundamental characteristics in Pytorch.
And if you're listening to this podcast and I'm going this far, only know a thing or two about it, but let me just take some time to explain, you know, what is so important about batching.
The concept behind batching is that when we do operations in Pytorch like adding or subtracting or multiplying, we don't do them on single numbers.
and said, we do them on batches, on arrays of numbers.
And when we do an operation, we do it many times over.
In a deep learning context, when we do a batch computation, we might be doing the same operation, doing the same series of matrix multiplies, convolutions, whatever on multiple inputs all at once called a batch of inputs before, you know, computing a loss and doing stochastic gradient descent in the situation.
Batching has a long history.
The concept of computing on arrays or vectors comes from all the way back from this language called APL, where everything was an array, and you you sort of only could ever do operations on it.
APL's concept of defining operations that worked on arrays rather than single elements sort of paved the way for most modern numeric computing libraries Pytorch included.
The most important thing about batching is it lets you amortize the overhead of whatever interpreter loop or top level programming language you're using.
because when you ask for an addition or multiply, you're not just doing it on one element.
You're doing it on many many elements.
And so if your batch size is large enough, if your array is large enough, then while you're going to spend most of your time in some sort of optimized C code that's handling the actual processing for each element, rather than, you know, wasting all your time in the interpreter, you know, repeatedly looping over something.
So, you know, like basically at a higher level obstruction, If you write code that operates on, you know, arrays rather than on single elements, we can just do a lot better job at executing it eagerly.
This characteristic of batching shows up all over the place.
For example, in the automatic differentiation community, prior to the rise of deep learning, many AD systems would actually, you know, perform AD at the level of individual operations on single numbers.
And well, this would actually lead to quite a lot of performance problems because well, you know, you're tracking these fine grained, you know, autograd histories through every single element in, you know, maybe some sort of physics simulation.
And so when we do autograd in PyTorch, we actually track automatic differentiation, the level of bathed operations, not individual operations, and that reduces a lot of the record queuing we have to do because, well, if you have a ten thousand size array, we still only record one piece of information for the autograph of operations involving that array.
Given the importance of batching for running code efficiently, you might imagine that it would be easy to write batch code in PyTorch.
And well, you'd be half right.
So in a previous podcast episode, I talked about vmap, a new feature in PyTorch for taking code that's written in a per example way and converting it into its batched version without requiring any changes.
VMAP is pretty cool.
You should go listen to that podcast if you're interested in it.
But, you know, people were writing Pyturgical models way before the day is a VMAP.
and there were, you know, simpler ways of writing batch computations.
Namely, you just took operations that knew how to handle batch operations and you strung them together.
And so if you were just doing simple operations like, you know, point wise operations, this wasn't too difficult because, well, if you add together a tensor of size two with another sensor of size two.
That's the same thing.
If you turn it into a batch where you'll take a tensor of size n by two, and add it to another tensor of size n by two.
Nothing changes in the way you write the operation in this situation.
But it's a little too much to ask for every cartridge operator to work in the same way.
And in fact, if we look at all sorts of operators and we try to classify what they're behavior is, you'll quickly find that there is a lot of variation.
So there's a few cases that are very regular.
So one is this point y situation.
Right? And in fact, there's a sort of more general case of this, which are just functions that take arbitrarily many batch dimensions and functions that are willing to broadcast.
Broadcasting, by the way, is this thing where if you don't provide enough elements compared to someone else's batch we will draw so called, broadcast the element.
Namely, we'll we'll stamp out multiple copies to sort of match up the size in question.
This is really useful.
For example, if you have an array, you wanna add two to it.
Well, two is not the same size as a, you know, ten by ten array.
but we'll just broadcast two into a ten by ten array that just contains a lot of twos and then add them together.
So functions that, you know, take many bashed mentions and are willing to broadcast, these are typically just the point wise functions.
And these are very well behaved and it's very easy to, you know, do batch computations with them.
Some functions, however, only take one batch dimension, and you're going to have to actually kind of look at the documentation to figure out if this is the case or not.
there really isn't any rhyme or reason.
A lot of this behavior is simply inherited from the old days in Louis towards where, you know, like, someone was writing the kernel and it was for neural networks and, you know, usually you only have one batch dimension in neural networks just the batch of the inputs you're processing over.
they didn't really need more batch dimensions.
So you'll have some functions that only take one batch dimension.
Some of these functions, you know, are even like they will take an optional batch dimension.
So if you just leave that dimension off, they'll just assume that you just wanted to operate on a batch size of one.
And some functions are really weird, like, take for example, torch dot matmall.
Depending on the dimensions size of each of its inputs, It might do a matrix multiply, it might do a dot product, it might do a matrix vector product, or it might do some sort of batch computation and there there's like a bullet list saying what happens in each of these situations? So it's no surprise people really like using vmap because well, vmap just sort of abstracts all this information away.
But, you know, we have to pay the piper somehow.
And so the cost of implementing v map we actually had to write all of these batching rules to, like, figure out, you know, how exactly to put things together.
And I talk a bit more about that in the vmat podcast.
What I wanna talk about today is I wanna compare and contrast batching operations with how it's done in NumPy.
Because in NumPy, actually over the years, NumPy has developed a little more structure on batching and broadcasting operations, and they call these the structure you function.
And I just wanna explain what a you function is because it's a pretty useful concept and all of API concepts from PyTorch were taken from Nine Pi.
We don't actually have a direct concept of ViewFunk, but it's one of the things we're considering adding in the near future.
So a u function or universal function for short is NumPy's way of referring to any function that, you know, has a number of well defined properties that make it work very regularly.
And what do I mean by that? Well, you folks are functions that have batching behaviors.
So that is to say you can add more dimensions to the big game and you can, you know, broadcast them if the dimensions don't line up exactly.
And they also support some amount of typecasting.
So if you pass in some inputs, that don't have exactly the same types.
Youfunk will know how to promote the type and, you know, like, get some common type to do the computation in.
So the concept behind the ufunk is really just, you know, taking some primitive operation like an add between two elements and then turning it into a vectorized operation.
that has that can actually operate on as many dimensions as you want.
And if this sounds familiar to you, it should because tensor iterator is basically an implementation of, you know, turning c plus plus functions into what are basically you function in matrix.
So we don't call them you function And, you know, actually, you folks in NumPy have a bunch of other interesting properties for one.
They have a bunch of other variants.
So when you talk about numpy dot add, there's actually also a numpy dot add dot reduce.
And what that function does is it's a function attribute on side of the Numpy dot add function is it takes, you know, your reduction dimension and reduces it using the operation that is the one from the u function, mainly edition.
So how come u function aren't just an internal implementation detail in NumPy.
I mean, you know, Tensor iterator is something that you have to know about if you're a Pytosh developer, but it's not a user visible concept.
I asked Ralph Gommerz, a NumPy maintainer and one of our collaborators at Konsaipe about this, and he gave me some very interesting information about eufeng's.
So new functions are not that great for users because users find it a little strange to take a function then take an attribute on it and then say, n p dot add dot reduce.
That's kind of weird.
But because you folks are introspectable, and, you know, they have a very regular structure.
They can be used in other libraries to do things that, you know, sort of wouldn't be possible.
with just plain numpai.
So for example, sci fi dot special consists mostly of eu function's that are easy to define, and they just reuse Numpy's machinery to take these, you know, functions and then turn them into you folks.
In the same way that, you know, Downstream uses a pie torch might want to use tensor iterator to, you know, make point wise style operations.
But another example of eufunks is number.
So number is a optimizing compiler for Python that basically, you know, can take code that is just written in Python and then compile it to CPU or CUDA.
And so when you write a Numpy operation and it is a eufungal operation, well, number can actually easily lower that into their IR because they know, hey, well, you folks all operate the same way.
So if it's something that you phone, it just needs to know, you know, what single element operation is, and then otherwise can use a common lowering behavior in a situation.
One of the reasons why I person have been thinking about Numpy You Fungs recently is because we're looking at how to rationalize our operators and sort of reduce the amount of boilerplate people have to write in a situation.
And, you know, actually exposing this concept of you function as a concept in our operating library is one way of saying, hey, all of these operators have the same behavior so you can treat them in the same way.
In fact, there's an even more general concept than you folks called a generalized you function.
And these you folks basically make it possible to define things that aren't just element wise operations like Adam subtract, but things that actually do nonjurial transformations on dimensions, like matrix multiply.
And the concept is still kind of the same.
You need to define what the, you know, sort of, core operation is, right, like a matrix multiply.
takes your dimensions and removes the inner dimensions and, you know, puts the outer two dimensions together.
But then once again, because we're in a batch universe, you might wanna actually batch this operation.
And so the generalized view function says, okay.
And then, you know, you can tack on as many batch dimensions as you want.
And so once again, if something is a generalized u-funk, then you know at least that the batching is handled in a very regular way.
So, you know, the combination of these two things means that, you know, it's not as important to have something like v map because, well, as long as your operators are one of these things, then, you know, you can rely on it acting the same way.
Although, well, it's still kinda nice having vMAb because not everything is gonna be eufunk.
Not everything's gonna be generalized, you funk.
And so, you know, if you just don't know, if you don't have time to read the documentation, you know, v mAb will just make it easy.
You just don't have to worry about it.
So that's it for batching.
So batching is how we make Pytorch as an eager library efficient because we can amortize the overhead of Python.
over doing combinations over many many elements.
Pytorch is not very regular about how batching is done on operators.
It's a very per operator thing.
some operators take many batch dimensions.
Some operators only take one batch dimension.
Some operators don't take a batch dimension at all.
But there is some structure to our operators and one way to think about it is is an operator implemented using tensor iterator or not.
But some other ways of thinking about it because Pytorch is very similar to NumPy is, you know, what things are you funks? What things are generalized you funks? That's everything I wanted to say for today.
Talk to you next time.
.
