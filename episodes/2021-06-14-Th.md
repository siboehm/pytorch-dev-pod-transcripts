---
layout: post
title: "Th"
date: 2021-06-14
---
The original audio files are available at https://pytorch-dev-podcast.simplecast.com/

# Th

Hello, everyone, and welcome to the Pytorch dev podcast.
Today, I want to talk about t h, the previous library that was used to implement all of the kernels in Pytorch.
This is something of a historical episode because there isn't really that much code that is still in t h and Pytorch today.
but it's still a kind of interesting historical example to look at and we still do have some t h code.
So if you're an unlucky person, and you're, say, trying to add a new d type or trying to deal with our storage Python bindings, you might still need this knowledge one way or another.
So that's what this podcast is gonna be about.
So what is t h? Well, as I've mentioned in previous podcast episodes, Pytorch is not a project that was written entirely from scratch.
It took all of its code from Louis towards the previous iteration of the framework, which was a bunch of c code bound to Louis.
So we kept the c code, that's the t h library, and we bound it to Python instead of Lua, and that's how Pytorch came into being.
When you ask the question, why do we wanna port all of our t h code to c plus plus? We have to understand a little bit about how t h was put together in the beginning.
And the most important constraint for the construction of the t h library is that it was written in c.
This posed a challenge for the library in a few ways.
One is that The Tensor Library TH needed a way to write algorithms that would be generic over multiple d types like suppose you're writing a matrix multiply and you wanted to work for both floats and doubles.
In c plus plus, you could use a template to templatize over the d type in question and then instantiate code multiple times for each version of the d type you want.
But in c, there's no such mechanism.
Right? Like one of I had a friend in grad school who was like, yeah, c plus plus is a really terrible language, but, like, it's really convenient to be able to have a reusable vector container that you can use on different types.
And so t h had this problem.
The problem was they wanted a bunch of tensors that were for different d types, but there was no good way to actually write them all out without actually having to write out all the code, you know, n times where n is the number of d types.
in your code.
And so the way t h decided to solve this problem and also the reason why t h is kind of universally loathed and something that we're trying to get rid of, is that it decided that the problem could be solved with macros.
So here's how t h decided to solve the problem.
Let's imagine that you're writing some c code for an algorithm, same multiply and you want to write it in a way that it's generic over the d type in question.
So instead of writing float or double inside your program, you instead write a scaler.
You say, okay.
Well, everything is some unspecified scaler type and don't ask me how it's gonna be defined, but it'll somehow be defined.
and you write your code all in this generic way.
When you write functions that should be externally visible, you also use another macro, the t h tensor underscore macro.
to say, hey, I'm defining a generic function.
I don't know what its name is.
I'm gonna tell you what the name of it is later.
So where we're where we're going here is we're gonna actually give macro definitions for scalar and for t h tensor underscore that basically expand these to the appropriate thing.
So if you're doing a float tensor, then the scalar will be a float and t h tensor will become t h float sensor.
But if it's a double, then scalar will be double and t h tensor will become t h double tensor.
And then the trick is We have the C code.
It refers to all these macros, and what we will do is we will define the macros to be float include the c file.
This is very unconventional.
Right? Normally, you only include header files.
But here, we're actually gonna include the honest goodness c file.
include the z file, and then undeuff the macros, redeff them to the next d type we wanna ins instantiate with, and then include the C file again.
And we'll keep including the C file with different settings of the macros until we're done instantiating all of the d types that we want.
So, yeah, there you go.
This is the most important thing to know about t h and in terms of code structure, all the C files that are instantiated multiple times in this way, they live in generic slash folders.
And these aren't all in t h or t h c.
There's also a folder in towards C cert for doing python bindings that also is written this way.
And so, you know, whenever you see the generic folder, that just means it's the c code, c plus plus now because we made it into c plus plus that gets stamped out multiple times in this way.
Doing things this way also meant that it was easy to generate a new tensor type for every instantiation.
We had a struct t h float sensor and a struct t h double tensor, etcetera, for each of these things.
And those were the Those were also instantiated in the same way.
And this also caused some problems when we wanted to write generic code because well, there's these structs are all different in all of these cases.
And so one of the things that we did early on when porting to c plus plus is we unified all of these different d type structs into a single struct that was polymorphic.
Because, well, we don't actually need to store floats or doubles directly in the Strip.
We only ever store a data a pointer to the data in question.
so that's something that you can easily write a single struct that works in all cases for.
I actually don't think this macro instantiating strategy that the old t h libraries was too bad.
It's actually a pretty nice way of adding on a fake parameterization system to a language that doesn't need to fully support it, aka c.
And I don't really I can't really think of other ways you could have gone about doing this.
Actually, My PhD thesis at Stanford was about backpack, which was this module system we retrofitted onto another programming language called Haskell.
And it also operated and by very similar ways, you had a bunch of sort of types and functions that you left unspecified, and then you instantiated them with an actual implementation later when you wanted to do the coding question.
And why did we do it this way? Well, we did it this way because we didn't really wanna make major surface changes to the language in question.
So it turns out you could do my PHE thesis and see just with macros.
Who knew? There's a few other things about a t h code that are good to know, although they're less major than this macro system.
So one is that t h because it's written in c, has to be manually rough counted because you don't have a concept of constructors or destructors, which c plus plus programs use to implement RAII.
RAII is probably one of the other sort of killer features of c plus plus because who wants to do manual reference counting.
It's also a big problem though because with with automatic reference counting, you can't easily tell when you're doing these ref counts.
And so it's easy to write code that does a lot of unnecessary ref count.
So, you know, a double ish sword.
Right? Like, when you wrote t h sensor code, it was easy to get the rep counting wrong, but at least you could see it all in one place.
And then when it's you know, all implicit and hidden away in these classes.
It's easy for people to forget, oh, yeah.
There's actually cost to rough counting bumping willy nilly.
I guess this is one of the reasons why linus Torvalz still writes all of Linux in c because Siebel's plus is just this terrible language that, like, has all of this you know, extra stuff that happens automatically and it's easy to forget about and you write really slow code.
Anyway, so if we had a manual graph count in sea and that was also a pain and it was especially painful when you had ever conditions because you had to make sure you freed all of the temporaries when the error credentials fired.
Because we were we were actually in the old days, when it was c only, we were we would crash the process when you hit an error like this.
but very early on when we started porting things to c plus plus, we were like, okay.
We're gonna do everything at c plus plus.
And then when you hit an error, we wanna raise an exception.
So we couldn't recover it from it and not just crash your Python process when this happens.
one last thing that's interesting about t h and actually sort of has propagated its way to our agent ports is a lot of the neural network operations that we supported have a lot of buffers that get passed from forwards to backwards.
So what are these buffers? Well, basically, they're extra outputs from a function in the forward pass that you don't actually use like from the perspective of a user, these buffers are invisible.
You don't see them.
They just invisibly get passed to the backwards function where they get used.
And a lot of the times, they don't actually do anything useful.
They're just like scratch space that the kernel in question uses.
Why do these buffers exist? Well, it turns out that back when we were in Louis Vuitton, we didn't actually have a caching allocator for CUDA.
So allocating CUDA memory was very slow and it was very expensive.
and one of the, like, first new pieces in Pytorch that also was one of the really important pieces for making our CUDA programs run fast was adding a caching memory allocator.
So that so in lieu of, you know, you really wanted to not have to allocate memory Willy nilly.
So if you allocated this buffer and then you saved it for later, that was actually a benefit because you wouldn't have to do this allocation again later.
Piters doesn't have this requirement.
So if you ever see these scratch buffers being passed around, that's just useless memory usage and you should just get rid of So that's really all you need to know about t h.
I'm not gonna labor on because we have a process of porting t h operators to a ten operators.
that has gone pretty far.
We're very, very close to getting rid of all of the legacy t h code.
And no one else is gonna have to have the c code inflicted on it.
There's also there was also a lot of legacy code gen that was written specifically for the c library.
We've also gotten rid of all of that.
you don't really have to worry about that anymore as well.
There's one thing that I regret a little about porting all of our t h code to a ten.
and that's the loss of static typing in call sites.
One of the things that is kind of expensive in modern PyTorch is when we patch.
So we have to go look at all the tensors and figure out, oh, is it CPU Acuda and go to the right one for the right d type in that situation.
TH didn't have this problem because there was a separate type for every t h float tensor, t h double tensor, etcetera, and you always wrote code knowing exactly what your d type was.
Right? Because everything was in one of these c files where you're gonna instantiate the macros.
So calls in t h, while they couldn't get in line because in lining isn't really a thing in c, you could still actually just compile them as normal function jumps without any fuss and fuss.
And we have swung back around to wanting to be able to do this in PyTorch proper when performance matters, but it's a bit harder because, you know, we don't have we don't really wanna template all our code.
And so It's just kind of annoying to actually make sure these things work.
One thing we've been looking at is maybe we can use very small amount of, you know, just in time compilation techniques no, not the jit compiler for Pytors, but like good old fashioned polymorphic inline caches that might make it possible to, like, speed this up.
But that's something just speculatively with we've been looking at.
Okay.
So you know about t h.
That's everything I wanted to talk about today.
Talk to you next time.
.
