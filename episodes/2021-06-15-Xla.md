---
layout: post
title: "Xla"
date: 2021-06-15
---
The original audio files are available at https://pytorch-dev-podcast.simplecast.com/

# Xla

Hello, everyone, and welcome to the Pytorch Step podcast.
Today, I want to talk about torch XLA, Pytorch's integration with TensorFlow's XLA compiler.
What is XLA? XLA is a optimizing compiler that sits below TensorFlow, our most favorite deep learning framework, and it's sort of purpose in life is several things.
So one is it's intended to have a lot of optimizations The idea being that XLA's intermediate representation h l o i r doesn't need to have that many operations.
and then XLA will know how to compile them and optimize them in a way even though you may have expressed your programs as having lots of, you know, antibody operations.
Exelay is supposed to be able to fuse them together and give you good good performance.
The second thing Exelay is supposed to do, and the one that really made Python want to integrate with x-ray is it's the only stack that can actually run Google TPUs.
CPUs are a deep learning hardware accelerator that is built by Google.
There's a lot of free GPUs.
Google loves getting people to use them.
And if you wanna use them, you've gotta use XLA.
And so if you wanna use PyTorch and use TPUs, while torch XLA, is your guy.
Toric Exelay has a lot of people who have worked on it both on Facebook's side and on Google's side and two Big people I wanna call out who were very historically important to this development are Aileen Zhang on the Facebook side.
and Davide Laubez on the Google side.
They both have very big influence on the XLEA project.
Okay.
So how exactly does torch x lay work? There's a big big problem when you wanna take PyTorch which is a eager ostensibly an eager remote framework and hook it up to XLA, which which is a graph compiler.
It takes in a graph of operators that is already preset and compiles it into some efficient form, which is that Pytorch eager doesn't actually have any graph representation.
Well, see my previous podcast talk about torch grip, where if you torch grip to your model, then you can get a graph mode representation of it But one of the things that going into the XLA project, they wanted to make possible, was they wanted it to be possible for people to take good old fashioned Pritchard eager scripts.
and run them straight on x l a without having to do very many modifications.
In other words, the goal of this integration was functionality.
We wanted, you know, to have as much stuff work as possible on XLE with as little work as possible that you wanna do.
This is a double edged sword.
I'll talk about it more later.
in the podcast.
So how can you run eager mode code directly while still feeding it to a graph compiler? Well, the big idea that everyone comes up in this situation is to use some sort of lazy tensor.
Now, I don't mean lazy in the sense that you had a tensor and you were going to materialize it at some point, but you're just waiting for someone who actually needs it to use it.
Exelay is Toric Exelay is a really, really lazy operation.
What it does is it doesn't run anything when you run your model until the very end when you wanna do the optimizer step.
And that's the point when the trace of operations that you've run during this period of time actually gets sent to Exelay and gets compiled or, you know, hopefully, we've already seen this trace before because your program maybe isn't too dynamic and that's when it gets compiled into XLA and done.
So what are we doing? So you're running a bunch of Pytorch ops.
It all looks like normal Pytorch, but under the hood, we're constructing XLAs HLOIR, and at the very end, that's when we actually send it to XLA.
Lazydays tenders in this way are very reminiscent of dinette, another framework where the idea was you wrote your c plus plus code, you ran it, you ran every single example one by one, and then they had this thing called automatic batching.
So they'd look at all the traces and then batch them up so that you could run them more quickly.
So torch x lay also runs very much this way when you run your Pytrich program for a single iteration, we always are constructing the h flow graph from scratch every time around.
So Actually, XLA does support dynamic execution.
Right? Like, if you do something a little different on the next run, you'll just get a slightly different h l o i r and we'll just compile it to some other thing in that situation.
So what exactly goes into making a lazy tester work? Well, there's a few very important things.
So one is that you need to interpose into the calls, into PyTorch when you when you call a bunch of operators.
And where this inner position happens is pretty important because we want to also do training on x l a.
And that means we need to be able to differentiate our graphs.
and torch excellent takes the approach of reusing Pytorch's autograd engine.
And because it reuses the autograd engine, what we need to be able to do is we need to run our program lazily on the forward pass and then also lazily run the backwards pass to generate the operations for all the things that the automatic differentiation needs, and only then run the entire code in question.
So the torch x lay gets integrated in the dispatcher because the dispatcher is the point that's low enough in Pytorch's stack of functionality to observe both the forward and backward forms of the eighty pass.
Okay.
So once you get to the bottom, the XLA dispatch key, that's just what processes tensors that are XLA of the XLA device.
So we go into Torsheim XLA And what George externally does is it takes all the arguments and figures out how to construct a corresponding HLO IR node for the Pytorch operation that was done.
So basically, there's a translation of the Pytorch semantics into the terms of the XLA semantics.
And you might imagine that we would construct h low IR directly at this point in time, but that's not quite what happens.
What actually happens is there's a intermediate IR that gets built by torch x l a and it's intended to be very fast to build And then once we're done, we first check if, you know, this IR magic something exactly that we've seen before.
And if that's the case, then we don't need to do any compilation.
We don't need to translate into x l a h l o r.
We can just directly pre reuse the pre computed trace.
Otherwise, they do a very simple elaboration into XLIR.
And this this just makes it possible to run XLA programs pretty quickly even if XLAs HLOIR isn't designed to be, you know, built very quickly and repeatedly in this way.
And that's really it.
So most of Toric XLEI is just the massaging of, you know, Pyturgical operators into XLEI form.
know, inserting the things that you need, you know, smoothing over differences and semantics, but, you know, deep learning frameworks are all very similar.
So in a lot of cases, things match up pretty closely.
There is one place though that things don't match up very closely, and that's Pytorch's support for views.
Recall, Pie George supported for views means that if I have a tensor, I can take out a view on that tensor.
And then no matter if I mutate the view, or the original base tensor, the change is reflected in the view or the base respectively.
So axeloid doesn't actually natively support this.
It has some support for aliasing and mutation, but not to the degree that Pytorch does.
In other words, it doesn't really know about strides.
Strides are a very Pytorchy, torchy, lua torchy, torch seven, you know, lineage thing.
So how exactly do we translate these Pytrich programs to XLA? Well, what we do is the functionalization pass that I talked about in one of my earlier podcast episodes.
So what we do is we keep track of all the aliases when Pytorch makes them And then when someone updates an alias, we just go and look at all the other aliases and reapply the update in those cases.
and this happens lazily so that we don't actually have to keep track of all the aliases that are on the Tensor.
This works pretty well and so you can mutate to your heart content, and we are still able to translate to Exelay.
I mentioned earlier that Toric Exelay's integration favors functionality or performance.
And another way that this is favored is that x l a has a CPU fallback.
because Pytrich has a ton of operators and XLA HLO while cool doesn't have that many operators.
One of the selling points of HLO IR is it's pretty small and it's easy for back ends to target.
Actually, that's why a lot of, you know, of the burgeoning new hardware accelerators often target XLA because that's a very easy place to start.
And well, you know, if you do XLA, then you've got TensorFlow, and TensorFlow is a very important framework to support when you're doing this sort of thing.
So Pytorch XLA has a fallback.
So what the fallback says is if there's some op and we don't know how to translate it to h low i r, we'll just go ahead and immediately run the pixelated graph to get out what the output would be translate that output into a PyTorch CPU Tensor, and then run the good old fashioned PyTorch CPU operation, and then go back and put it back into the x leap graph.
So, you know, that's not gonna be very fast.
Right? Like, you know, you don't you're seeing less of the graph to optimize and you have to, you know, go ahead and, like, If you were on t p u, you have to move it back to CPU so you can do the fallback.
But at least your program runs.
And in a lot of cases, that's all you need.
You you you didn't care that much about performance.
You just needed to get it working in the first place.
That being said, sometimes all of these conveniences can make it hard to make your torch x-ray models go fast.
So we've had some experience working with people who wanted to get their stuff running on GPU.
And one of the themes that happened is that sometimes their code would just run really slowly.
And why was that? Well, oh, okay.
There was, you know, a if statement somewhere inside their model and that was causing torch x lay to have to recompile many, many different traces every time it went one way or the other in the if statement.
And, yeah, you have to, like, rewrite your model a little so that the traces don't change over time so that you can reuse the XLEA traces.
And that can be a little challenging.
It's a bit different than say jacks where jacks provides you this jit commuter and what the jit commuter says is you're gonna run the jit commuter once on this model that you're gonna run, and whatever it is that you traced at that point in time, that's what you're gonna have compiled.
So there's no expectation that things are gonna work dynamically.
There's no expectation that, you know, every time you go through a new batch, you're gonna jit again.
Like, you know, obviously, you jit once and then run it many times.
for better or or worse.
Right? Okay.
I wanna talk about some nuts and bolts about General Pitcher's development.
you might have had your eyes glaze over because you've never, you know, interacted with XLA and whatever, like, do I have to care as a PyTorch developer? And the answer unfortunately is yes because XLEA is in our CI and so if your PRs are not passing XLEA CI, well, we are not going to let you land them.
That being said, there are some peculiarities to the XLECI.
XLEC lives in a separate repository because we have a lot of Google people who work on it.
and they only commit access.
So it's in a separate repo from Pytorch Pytorch, which only Facebook people can directly land to.
So how did we set up the CI? Well, there's the right way to do it, and we did it the wrong way, but it was pretty easy, which is Pytorch will pull whatever the master build up x ideas at any given point in time for your peers.
Crazy.
Right? Like, you're never supposed to do that in CI.
But that's what we do.
And what makes it work is we have a lot of dedicated people on call for x-ray, like Aileen, like Jack Sowell, who When someone has a PR that's making a change in an operator and that operator is affecting SLA because there's some translation in SLA, and now it's changed and it needs an adjustment.
You can just sort of send up the bat signal and be like, hey, you know, we need some extra layer work.
and usually an XLAPR will show up, you know, in short order.
And then what just needs to happen is you land the Pytters PR and then once the Pytarg peers landed, the XLA PR is landed as well.
The XLA's CI has some pretty nifty features.
For example, they have this thing called torch pen.
So, like, if you're making an x l a change and it needs to be against a specific pull request from Pytorch, well, you add this Torchpin magic file that says a PR name.
And then when your CR runs, they'll be run with respect to that Pytorch's pull request and not master in that situation.
And, yeah, sometimes this means that we break the x lay build temporarily when things land.
And usually, if that happens, you just are like, hey, you know, is there an x lay change? And usually, there is so the x-ray change lens, and then everything's clean again.
That's really the most important thing.
Like, just knowing who to talk to to resolve x rays errors and someone will help.
Don't worry.
You don't have to know everything about XLEA.
There's also some cool stuff coming up in the space of XLEA integration.
So one thing that Brian Hersha has been working on is an external code gen in Pyturg, Pyturg that XLA can use.
And we've actually landed most of the pieces of this.
Previously, XLEA actually had its own sort of homebrew code gen with a homebrew parser for native functions dot YAML.
that generated all of their definitions because there were a lot of boilerplate to write, especially with CPU fallback.
Right? because every operator needs to have a CPU fallback, and it's very, very boring.
You just translate all the tenses to CPU, run the CPU operation, translate them back to x l a.
So we have a shiny new co gen in Pytorch and we've been one trying to make it possible for people outside of Pytorch.
Pytorch to make use of our co gen and also provide a much nicer, you know, back end generic mechanism for overwriting operators in the way Exelite wants to.
Because actually, what has happened is Exelite is our most famous and most successful back in extender of Pyturbine, and people were actually copy pasting Exelay's kind of janky co gen for their use cases.
So Brian's got this new thing.
It's pretty cool.
We're working on moving the users from n equals one to n equals two, and there will soon be lots of documentation and more pitches about it.
Another cool thing that's coming up is Alex Suhan has been working on refactoring torch x-ray into what we're calling the lazy core.
because XLA is this lazy tensor functionality, which, you know, like, records what functions got run when you're doing in Pytorch.
And this is something that a lot of other backends want to use as well.
Right? Because anytime you have a graph back end that can't run things in eGromote, By the way, don't do that.
Like, hey, hardware accelerators, support e remote, support streams.
It's a good idea, really good programming model.
but let's say that you can't.
Right? Well, you need something like Exelay's infrastructure for recording the graph so that you can actually run it.
And so La ZCore is the part of XLEA that, you know, doesn't have any of the XLEA loadings, but has that generic infrastructure for actually recording laser tensors.
And so he's got a branch that which has split these out into two pieces, and Brian and Alex are working on merging this into Pritchard Core so that generally people can use it.
Okay.
So that's a whirlwind tour of x l a.
That's everything I wanted to say for today.
Talk to you next time.
.
